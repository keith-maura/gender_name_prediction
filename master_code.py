# -*- coding: utf-8 -*-
"""Master Code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pOS4xZkMl2_U6rJZp7SCUtOVqphh5YIX

# Imports and Google Drive setup
"""

# Google Drive imports
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

# Data editing and visualization imports
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Scikit-learn model imports
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import LinearRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier

# Scikit-learn cleaning and analysis imports
from sklearn.model_selection import train_test_split
from sklearn import metrics
from sklearn.model_selection import GridSearchCV

# Keras imports
from keras.layers.core import Activation, Dense, Dropout
from keras.models import Sequential
from keras.preprocessing import sequence
from keras.layers import Dense, Embedding, LSTM, Bidirectional, Input, Conv1D
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import plot_model

# Google Drive authentication
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
gdrive = GoogleDrive(gauth)

"""# Data Cleaning and Feature Engineering

## Feature engineering functions
"""

# Function to define a new column for the given dataframe
def define_column(col_name, funcn, df):
  df[col_name] = df['Name'].map(lambda x: funcn(x))

# Function to count how many of the given letters are in the given name
def count_letters(letters, name):
  count = 0
  for letter in name:
    if letter in letters:
      count += 1
  return count

# Function to encode letters into integers
def letter_replacer(letter):
  return 'abcdefghijklmnopqrstuvwxyz '.index(letter)

# Add feature columns
def add_feat_columns(df):
  define_column('Length', len, df)
  define_column('First letter', lambda x: letter_replacer(x[0]), df)
  define_column('Last letter', lambda x: letter_replacer(x[-1]), df)
  define_column('Number of vowels', lambda x: count_letters('aeiouy', x), df)
  define_column('Number of consonants', lambda x: count_letters('bcdfghjklmnpqrstvwxz', x), df)
  df['Ratio vowels'] = df['Number of vowels'] / df['Length']
  df['Ratio consonants'] = df['Number of consonants'] / df['Length']
  for letter in 'abcdefghijklmnopqrstuvwxyz ':
    define_column('Contains \'' + letter + '\'', lambda x: 1 if letter in x else 0, df)
  return df

"""## Harvard dataset cleaning"""

# Pull in name data
file_downloaded = gdrive.CreateFile({'id': '1CeijIR6Zdi51AJy1zMepozennloklGG6'})
file_downloaded.GetContentFile('wgnd_source.csv')
harvard = pd.read_csv('wgnd_source.csv').dropna(subset = ['name'])[['name', 'code', 'gender']].astype({'name': str, 'code': str, 'gender': str})\
.rename(columns = {'name': 'Name'})
# Get rid of gender = '?'
harvard = harvard[(harvard['gender'] == 'M') | (harvard['gender'] == 'F')]
# Make all names lowercase
harvard['Name'] = harvard['Name'].map(lambda x: x.lower())
harvard

# Function to check if name is in English alphabet or a space
def check_if_english_alphabet(name):
  letters = 'abcdefghijklmnopqrstuvwxyz '
  for letter in letters:
    if letter in name:
      return True
  return False

# Filter names for English alphabet and drop duplicates
print(len(harvard))
# Only keep names from English alphabet
english_names = harvard['Name'].map(lambda x: check_if_english_alphabet(x))
harvard = harvard[english_names]
print(len(harvard))
# drop duplicate names
# there are no duplicate names within a country!
harvard = harvard.drop_duplicates(['Name', 'gender', 'code'])
print(len(harvard))
harvard

# Names that show up for more than one gender
# Names have only one gender per country!
dups = harvard.groupby(['Name', 'code']).agg('count').reset_index()
dups[dups['gender'] > 1]

# Add feature columns
harvard = add_feat_columns(harvard)
harvard

# Country distribution (of countries with over 1000 instances)
countries = harvard.groupby('code').agg('count').reset_index()
countries_graph = countries[countries['Name'] > 1000]
fig = sns.barplot(x = countries_graph['code'], y = countries_graph['Name'])
fig.set_xlabel('Country code')
fig.set_ylabel('Count')

# Gender distribution
genders = harvard.groupby('gender').agg('count').reset_index()
fig = sns.barplot(x = genders['gender'], y = genders['Name'])
fig.set_xlabel('Gender')
fig.set_ylabel('Name')

"""## UC Irvine dataset cleaning"""

# Pull in name data
file_downloaded = gdrive.CreateFile({'id': '12OpLjUDMPNb3zMrAq4ho7IQva7SaY7BX'})
file_downloaded.GetContentFile('name_gender_dataset.csv')
irvine = pd.read_csv('name_gender_dataset.csv')[['Name', 'Gender', 'Count']].astype({'Name': str, 'Gender': str, 'Count': int})
irvine['Name'] = irvine['Name'].str.lower()
# Only keep names with at least 1000 occurrences
irvine = irvine[irvine['Count'] > 1000]
irvine

# Gender distribution of unique names
genders = irvine.groupby('Gender').agg('count').reset_index()
fig = sns.barplot(x = genders['Gender'], y = genders['Name'])
fig.set_ylabel('Name Count')

# Distribution of gender in all people sampled
genders = irvine[['Gender', 'Count']].groupby('Gender').agg('sum').reset_index()
fig = sns.barplot(x = genders['Gender'], y = genders['Count'])
fig.set_ylabel('Person Count')

"""### Binary name assignment


"""

# Take names that show up for both gender, select the gender it shows up most for
# Arbitrarily chooses male if there's the same number for both
dups_df = irvine.merge(irvine, on = 'Name').rename(columns = {'Count_x': 'Male Count', 'Count_y': 'Female Count'})
dups_df = dups_df[(dups_df['Gender_x'] == 'M') & (dups_df['Gender_y'] == 'F')].reset_index(drop = True)
del dups_df['Gender_x']
del dups_df['Gender_y']
dups_df['Gender'] = dups_df.index.map(lambda x: np.argmax([dups_df.iloc[x]['Male Count'], dups_df.iloc[x]['Female Count']]))
dups_df['Count'] = dups_df.index.map(lambda x: max(dups_df.iloc[x]['Male Count'], dups_df.iloc[x]['Female Count']))
dups_df['Gender'] = dups_df['Gender'].replace({0: 'M', 1: 'F'})
dups_df

# Names that only show up for one gender
names_no_dup = irvine[~irvine['Name'].isin(list(dups_df['Name']))]
print(len(irvine[irvine['Name'].isin(list(dups_df['Name']))]))
names_no_dup

# Concat into df (dup and non dup names)
names_binary = pd.concat([dups_df[['Name', 'Gender', 'Count']], names_no_dup], axis = 0).sort_values(by = 'Name').reset_index()
del names_binary['index']
names_binary

# Add feature columns
names_binary = add_feat_columns(names_binary)
names_binary

"""### Neutral name assignment"""

# Function to assign male, female, or neutral gender based on female ratio
def assign_gender(val):
  if val < 0.05:
    return 'M'
  elif val > 0.95:
    return 'F'
  return 'N'

# Take names that show up for both gender and assign gender
dups_df['Count'] = dups_df['Male Count'] + dups_df['Female Count']
dups_df['Female Ratio'] = dups_df['Female Count'] / (dups_df['Male Count'] + dups_df['Female Count'])
dups_df['Gender'] = dups_df['Female Ratio'].map(lambda x: assign_gender(x))
dups_df

# Concat into df (dup and non dup names)
names_neut = pd.concat([dups_df[['Name', 'Gender', 'Count']], names_no_dup[['Name', 'Gender', 'Count']]], axis = 0).sort_values(by = 'Name').reset_index(drop = True)
names_neut

# Examine gender counts
names_neut["Gender"].value_counts()

# Add feature columns
names_neut = add_feat_columns(names_neut)
names_neut

"""### Cleaning for continuous scale"""

# Take names that show up for both gender and retrieve female ratio
names_cont = dups_df[['Name', 'Count', 'Female Ratio']]
names_cont

# Retrieve male-only names
male_df = irvine[(irvine['Gender'] == 'M') & (~irvine['Name'].isin(list(names_cont['Name'])))][['Name', 'Count']]
male_df['Female Ratio'] = 0
male_df

# Retrieve female-only names
female_df = irvine[(irvine['Gender'] == 'F') & (~irvine['Name'].isin(list(names_cont['Name'])))][['Name', 'Count']]
female_df['Female Ratio'] = 1
female_df

# Concat results
names_cont = pd.concat([names_cont, male_df, female_df], axis = 0)[['Name', 'Count', 'Female Ratio']]
names_cont

# Add feature columns
names_cont = add_feat_columns(names_cont)
names_cont

"""# Logistic Regression for Binary Names

## Make the initial logistic regression model
"""

# Replace gender string with numbers
names_binary['Gender int'] = names_binary['Gender'].map(lambda x: 1 if x == 'M' else 0)
names_binary

# Test train split
# Need to omit 'ambiguous' column to test on Harvard set
X_binary = names_binary.iloc[:, 3:-1]
y_binary = names_binary['Gender int']
X_train_binary, X_test_binary, y_train_binary, y_test_binary = train_test_split(X_binary, y_binary, test_size = 0.3)

# Fit the model
lr_binary = LogisticRegression()
lr_binary.fit(X_train_binary, y_train_binary)

# An analysis function
def analyze_binary(model, type_str, feats, target):
  est = model.predict(feats)
  res = {'Set': [type_str]}
  res['Accuracy'] = [metrics.accuracy_score(target, est)]
  res['F1 Score'] = [metrics.f1_score(target, est)]
  res['Precision'] = [metrics.precision_score(target, est)]
  res['Recall'] = [metrics.recall_score(target, est)]
  res['ROC AUC'] = [metrics.roc_auc_score(target, est)]
  print(type_str + ' Confusion Matrix')
  metrics.plot_confusion_matrix(model, feats, target, display_labels = ['Female', 'Male'])
  plt.show()
  res['Sample Size'] = len(feats)
  return pd.DataFrame(res)

# Overview of results
results_binary = pd.concat([analyze_binary(lr_binary, 'Train', X_train_binary, y_train_binary), analyze_binary(lr_binary, 'Test', X_test_binary, y_test_binary)], axis = 0)
results_binary

"""## Test by country on Harvard set"""

# Replace gender string with numbers
harvard['Gender int'] = harvard['gender'].map(lambda x: 1 if x == 'M' else 0)
harvard

# Overview of results across all countries
harvard_results = analyze_binary(lr_binary, 'Harvard dataset', harvard.iloc[:, 3:-1], harvard['Gender int'])
harvard_results

# Analyze model by country
results_by_country = pd.DataFrame()
countries = harvard['code'].unique()
for country in countries:
  names_country = harvard[harvard['code'] == country]
  if len(names_country) > 500:
    results_by_country = pd.concat([results_by_country, analyze_binary(lr_binary, country, names_country.iloc[:, 3:-1], names_country['Gender int'])], axis = 0)
results_by_country

# Add country names to codes

# Read in country name/code data
country_codes = pd.read_csv('https://gist.githubusercontent.com/radcliff/f09c0f88344a7fcef373/raw/2753c482ad091c54b1822288ad2e4811c021d8ec/wikipedia-iso-country-codes.csv')\
.rename(columns = {'Alpha-2 code': 'Set', 'English short name lower case': 'Country'})[['Country', 'Set']]

# Merge with results
results_by_country = results_by_country.merge(country_codes, how = 'left', on = 'Set').sort_values(by = 'Accuracy', ascending = False).reset_index(drop = True).dropna()
results_by_country

"""# Linear Regression for Continuous Scale Names

## Make the linear regression model

### For all names
"""

# Test train split
X_cont = names_cont.iloc[:, 5:]
y_cont = names_cont['Female Ratio']
X_train_cont, X_test_cont, y_train_cont, y_test_cont = train_test_split(X_cont, y_cont, test_size = 0.3)

# Fit the model
lr_cont = LinearRegression()
lr_cont.fit(X_train_cont, y_train_cont)

# An analysis function
def analyze_cont(model, type_str, feats, target):
  est = model.predict(feats)
  fig, ax = plt.subplots()
  plt.scatter(target, est, color ='b')
  ax.set_xlabel('y actual')
  ax.set_ylabel('y predicted')
  ax.set_title(type_str + ' Set Results')
  ax.text(0.85, 0.05, r'$R^2$  = ' + str(round(metrics.r2_score(target,est), 2)), fontsize=8, color = 'r')
  ax.set_xbound(0, 1)
  ax.set_ybound(0, 1)
  res = {'Set': [type_str]}
  res['R2'] = [metrics.r2_score(target, est)]
  return pd.DataFrame(res)

# Overview of results
results_cont = pd.concat([analyze_cont(lr_cont, 'Train', X_train_cont, y_train_cont), analyze_cont(lr_cont, 'Test', X_test_cont, y_test_cont)], axis = 0)
results_cont

"""### For names that only show up for both genders"""

# Name data for both genders only
names_cont_both_genders = names_cont[(names_cont['Female Ratio'] != 0) & (names_cont['Female Ratio'] != 1)]

# Test train split
X_cont_both_genders = names_cont_both_genders.iloc[:, 5:]
y_cont_both_genders = names_cont_both_genders['Female Ratio']
X_train_cont_both_genders, X_test_cont_both_genders, y_train_cont_both_genders, y_test_cont_both_genders = train_test_split(X_cont_both_genders, y_cont_both_genders, test_size = 0.3)

# Fit the model
lr_cont_both_genders = LinearRegression()
lr_cont_both_genders.fit(X_train_cont_both_genders, y_train_cont_both_genders)

# Overview of results
results_cont_both_genders = pd.concat([analyze_cont(lr_cont_both_genders, 'Train', X_train_cont_both_genders, y_train_cont_both_genders),
                                       analyze_cont(lr_cont_both_genders, 'Test', X_test_cont_both_genders, y_test_cont_both_genders)], axis = 0)
results_cont_both_genders

"""### For names that are neutral (less than 95% majority one gender)"""

# Name data for neutral names only
names_cont_neut = names_cont[(names_cont['Female Ratio'] > 0.05) & (names_cont['Female Ratio'] < 0.95)]
names_cont_neut

# Test train split
X_cont_neut = names_cont_neut.iloc[:, 5:]
y_cont_neut = names_cont_neut['Female Ratio']
X_train_cont_neut, X_test_cont_neut, y_train_cont_neut, y_test_cont_neut = train_test_split(X_cont_neut, y_cont_neut, test_size = 0.3)

# Fit the model
lr_cont_neut = LinearRegression()
lr_cont_neut.fit(X_train_cont_neut, y_train_cont_neut)

# Overview of results
results_cont_neut = pd.concat([analyze_cont(lr_cont_neut, 'Train', X_train_cont_neut, y_train_cont_neut), analyze_cont(lr_cont_neut, 'Test', X_test_cont_neut, y_test_cont_neut)], axis = 0)
results_cont_neut

"""# Logistic Regression for Neutral Formatted Names

## Format data for models
"""

# Function to replace gender string with numbers
def assign_gender_int(val):
  if val == 'M':
    return 1
  elif val == 'F':
    return 0
  return 2

# Assign gender strings with numbers
names_neut['Gender int'] = names_neut['Gender'].map(lambda x: assign_gender_int(x))
names_neut

# View counts of gender
names_neut["Gender int"].value_counts()

# Test train split
X_neut = names_neut[['Name']].merge(names_neut.iloc[:, 3:-1], left_index = True, right_index = True)
y_neut = names_neut['Gender int']
# X_train_all and X_test_all have name as a column for analysis later
X_train_all_neut, X_test_all_neut, y_train_neut, y_test_neut = train_test_split(X_neut, y_neut, test_size = 0.3)
# X_train and X_test do not have name as a column, for actual model fitting
X_train_neut = X_train_all_neut.iloc[:, 1:]
X_test_neut = X_test_all_neut.iloc[:, 1:]

# An analysis function
def analyze_neut(model, type_str, feats, target):
  est = model.predict(feats)
  res = {'Set': [type_str]}
  res['Accuracy'] = [metrics.accuracy_score(target, est)]
  print(type_str + ' Confusion Matrix')
  metrics.plot_confusion_matrix(model, feats, target, display_labels = ['Female', 'Male', 'Neutral'])
  plt.show()
  return pd.DataFrame(res)

"""## Logistic regression"""

# Fit the model
lr_neut = LogisticRegression(multi_class="multinomial", max_iter=1000)
lr_neut.fit(X_train_neut, y_train_neut)

# Overview of results
results_neut = pd.concat([analyze_neut(lr_neut, 'Train', X_train_neut, y_train_neut), analyze_neut(lr_neut, 'Test', X_test_neut, y_test_neut)], axis = 0)
results_neut

"""## SVC"""

svc = SVC()
svc.fit(X_train_neut, y_train_neut)

# Overview of results
results_svc = pd.concat([analyze_neut(svc, 'Train', X_train_neut, y_train_neut), analyze_neut(svc, 'Test', X_test_neut, y_test_neut)], axis = 0)
results_svc

"""## Random Forest"""

rf = RandomForestClassifier()
rf.fit(X_train_neut, y_train_neut)

# Overview of results
results_rf = pd.concat([analyze_neut(rf, 'Train', X_train_neut, y_train_neut), analyze_neut(rf, 'Test', X_test_neut, y_test_neut)], axis = 0)
results_rf

"""### Hyperparameter tuning for random forest"""

# Hyperparam values to test
n_estimators = [50, 150, 500, 1000, 2000]
criterion = ['gini', 'entropy']
max_depth = [10, 40, 70, 100]
max_depth.append(None)
min_samples_split = [2, 5, 10]
min_samples_leaf = [1, 2, 4]
max_features = ['auto', 'sqrt']
bootstrap = [True, False]

# Create params grid
param_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,
               'bootstrap': bootstrap}

rf_tuned = RandomForestClassifier()
grid = GridSearchCV(estimator = rf_tuned, param_grid = param_grid, cv = 4)
grid.fit(X_train_neut, y_train_neut)

print(grid.best_params_)

# Optimized random forest
rf_optimized = RandomForestClassifier(bootstrap = False, max_depth = 40, max_features = 'sqrt', min_samples_leaf = 1, min_samples_split = 5,
                            n_estimators = 1000)
rf_optimized.fit(X_train_neut, y_train_neut)

# Overview of results
results_optimized = pd.concat([analyze_neut(rf_optimized, 'Train', X_train_neut, y_train_neut), analyze_neut(rf_optimized, 'Test', X_test_neut, y_test_neut)], axis = 0)
results_optimized

# View feature importances
feats = X_neut.columns
importances = rf_optimized.feature_importances_
importances_list = [[importances[i], feats[i]] for i in range(len(importances))]
sorted(importances_list, reverse = True)

# Top 3 names in each combination of actual/predicted classification

# Get predictions
est = rf_optimized.predict(X_test_neut)
# Make df with names, counts, actual and predicted classifications
analyze['Actual'] = y_test_neut
analyze['Predicted'] = est
analyze = analyze.merge(names_neut[['Name', 'Count']], how = 'left', on = 'Name')[['Name', 'Actual', 'Predicted', 'Count']]
# Group by actual and predicted labels, take top 3 in each
analyze = analyze.groupby(['Actual', 'Predicted']).apply(lambda x: x.sort_values(['Count'], ascending = False)).reset_index(drop = True)
analyze = analyze.groupby(['Actual', 'Predicted']).head(3)
# Replace numbers with actual labels
analyze = analyze.replace({0: 'Female', 1: 'Male', 2: 'Neutral'})
analyze

"""## LSTM Neural Network"""

def prepare_names(names):
  tk = Tokenizer(num_words = None, char_level = True, oov_token = "UNK")
  tk.fit_on_texts(names)

  sequences = tk.texts_to_sequences(names)
  data = pad_sequences(sequences, maxlen = 100, padding = "post")

  word_index = tk.word_index.items()

  return np.array(data), word_index

names = names_neut["Name"]
target = names_neut["Gender"]

prepared_data = prepare_names(names)
data = prepared_data[0]
word_index = prepared_data[1]
vocab_size = len(word_index)

embedding_weights = []
embedding_weights.append(np.zeros(vocab_size))

for char, i in word_index:
  x = np.zeros(vocab_size)
  x[i - 1] = 1
  embedding_weights.append(x)
embedding_weights = np.array(embedding_weights)

print(embedding_weights.shape)

target = pd.get_dummies(target).values
target

train_x, test_x, train_y, test_y = train_test_split(data, target, test_size = 0.25)

input_size = data.shape[1]
embedding_size = embedding_weights.shape[1]

model = Sequential()
model.add(Input(shape = (input_size,)))

# Character Embedding
model.add(Embedding(vocab_size + 1, embedding_size,
                    input_length = input_size,
                    weights = [embedding_weights]))
model.add(Conv1D(256, 7))
model.add(Activation("relu"))
model.add(Conv1D(256, 3))
model.add(Activation("relu"))

# Bidirectional LSTM
model.add(Bidirectional(LSTM(256, return_sequences = True), 
                        backward_layer = LSTM(256, return_sequences=True, 
                                              go_backwards=True)))
model.add(Activation("relu"))
model.add(Bidirectional(LSTM(256)))

# Dropout
model.add(Dense(128, activation = "relu"))
model.add(Dropout(0.3))
model.add(Dense(3, activation = "softmax"))

# Compile & Summarize Model
model.compile(optimizer = "adam", loss = "categorical_crossentropy", 
              metrics = ["accuracy"])
model.summary()

# Fit NN model
m = model.fit(train_x, train_y,
          validation_data = (test_x, test_y),
          batch_size = 128, epochs = 10)

# Plot train/val accuracy
plt.plot(m.history['accuracy'])
plt.plot(m.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

# Plot train/val loss
plt.plot(m.history['loss'])
plt.plot(m.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()